{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd \n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_titles(soup):\n",
    "    jobs = []\n",
    "    for div in soup.find_all(name='div', attrs={\"class\":\"row\"}):\n",
    "        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "            jobs.append(a[\"title\"])\n",
    "    return jobs \n",
    "\n",
    "def extract_companies(soup):\n",
    "    companies =[]\n",
    "    for div in soup.find_all(name='div', attrs={\"class\":\"row\"}):\n",
    "        company = div.find_all(name=\"span\", attrs={\"class\": \"company\"})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "        else:\n",
    "            sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "            for span in sec_try:\n",
    "                companies.append(span.text.strip())\n",
    "    return companies\n",
    "\n",
    "def extract_locations(soup):\n",
    "    locations = []\n",
    "    spans = soup.find_all('div', attrs={'class':'location'})\n",
    "    for span in spans:\n",
    "        locations.append(span.text)\n",
    "    return locations \n",
    "\n",
    "def extract_salaries(soup):\n",
    "    salaries = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={'class':'row'}):\n",
    "        spans = div.find_all('span', attrs={'class': 'salary no-wrap'})\n",
    "        if not len(spans) == 0:\n",
    "            for span in spans:\n",
    "                salaries.append(span.text.strip())\n",
    "        else:\n",
    "            salaries.append('Nothing_found')\n",
    "    return salaries\n",
    "\n",
    "def extract_summaries(soup):\n",
    "    summaries = []\n",
    "    spans = soup.find_all('div', attrs={'class': 'summary'})\n",
    "    for span in spans:\n",
    "        summaries.append(span.text.strip())\n",
    "    return summaries\n",
    "\n",
    "def extract_urls(soup):\n",
    "    urls = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={'class':'row'}):\n",
    "        d = div.find_all('div', attrs={'class': 'title'})[0]\n",
    "        url = \"http://indeed.com\"+ d.contents[1].attrs['href']\n",
    "        urls.append(url)\n",
    "    return urls\n",
    "\n",
    "def extract_dates(soup):\n",
    "    dates = []\n",
    "    urls = extract_urls(soup)\n",
    "    for url in urls:\n",
    "        broth = bs(requests.get(url).text, \"html.parser\")\n",
    "        for div in broth.find_all('div', attrs={'class': 'jobsearch-JobMetadataFooter'}):\n",
    "            for x in div.contents:\n",
    "                if \"ago\" in x:\n",
    "                    date = x.replace(\" - \", \"\")\n",
    "                    dates.append(date)\n",
    "    return dates\n",
    "\n",
    "def to_json(titles, companies, locations, salaries, summaries, urls, dates, search_location):\n",
    "    dicts = []\n",
    "    for i in range(len(titles)):\n",
    "        data = {}\n",
    "        data[\"title\"] = titles[i]\n",
    "        data[\"company\"] = companies[i]\n",
    "        try:\n",
    "            data[\"location\"] = locations[i]\n",
    "        except:\n",
    "            data[\"location\"] = search_location\n",
    "        data[\"salary\"] = salaries[i]\n",
    "        data[\"summary\"] = summaries[i]\n",
    "        data[\"url\"] = urls[i]\n",
    "        data[\"date\"] = dates[i]\n",
    "        dicts.append(data)\n",
    "    return dicts\n",
    "\n",
    "def list_page_soups(num_pages, query, location):\n",
    "    soups = []\n",
    "    for i in range(num_pages):\n",
    "        url = \"https://www.indeed.com/jobs?q={0}&l={1}&sort=date&start={2}\".format(query.replace(\" \", \"+\"), location.replace(\" \", \"+\"), i*10)\n",
    "        page = requests.get(url)\n",
    "        soup = bs(page.text, \"html.parser\")\n",
    "        soups.append(soup)\n",
    "    return soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_pages, query, search_location):\n",
    "    soups = list_page_soups(num_pages, query, search_location)\n",
    "    all_jobs = []\n",
    "    for soup in soups:\n",
    "        titles = extract_job_titles(soup)\n",
    "        companies = extract_companies(soup)\n",
    "        locations = extract_locations(soup)\n",
    "        salaries = extract_salaries(soup)\n",
    "        summaries = extract_summaries(soup)\n",
    "        dates = extract_dates(soup)\n",
    "#         print(dates)\n",
    "#         print(len(dates))\n",
    "        urls = extract_urls(soup)\n",
    "        print(urls)\n",
    "        json_data = to_json(titles, companies, locations, salaries, summaries, urls, dates, search_location)\n",
    "        all_jobs.append(json_data)\n",
    "    flat_list = [item for sublist in all_jobs for item in sublist]\n",
    "    main_df = pd.DataFrame(flat_list)\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pages = 10\n",
    "query = \"tech\"\n",
    "location = \"Chicago, IL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = main(num_pages, query, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0APcB5PE-vv4rq4lRUqBNRHh1T4fx-TP46vPzjjAkJXVvoBD-gi4c-kGVZQzThWHI6n0_w2OMzteHCVYtHEbelLOQdXYkf2euxcg9_CESWNP5oERiR9lpwbVZ_eKkv1MJZMoY0Zqy_U4U_VSM43YQkggwwCXZ5Ht0Rw4wQF9Dwtgs28OAS4l5B87o9A9TAOVIkrsR2hw4Ql6dpyAPTcntnRFrXWmP-60Kk52B4QNxneGv3t_rzDRVbFle5EbjoNgVDT-NmxbdJdFy60XhzbL6KwXYowGaGadKY6KZzzeVKXPyEJAtB0DbU6GSBfijSpVnCu67wUuDzzXzoP2kg-MQPaXRUAozpBtbu0ofxqZXX8S3uipEPkJXAnps1WFC9kth5YJ3kVcybQ77bdzMesMLEa1w38BoajS2IknFy0-ZS26A==&vjs=3&p=1&fvj=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
